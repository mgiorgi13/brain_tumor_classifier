{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgiorgi13/brain_tumor_classifier/blob/mattia/CNN_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDz3ICSorQXo"
      },
      "source": [
        "# **Importing Libraries**\n",
        "\n",
        "In this paragraph, we have imported the necessary libraries to create the CNN. TensorFlow and Keras are the main frameworks that we will use to create the convolutional neural network. Additionally, we have imported other useful libraries such as OpenCV (cv2) for image preprocessing, PIL for image manipulation, and Matplotlib for image visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7qv94seOl4RA"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation,Concatenate, BatchNormalization\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "from sklearn.utils import shuffle\n",
        "import zipfile\n",
        "import cv2\n",
        "import imutils\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from PIL import Image\n",
        "from matplotlib.pyplot import imshow\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wAp6lCRlmpYg"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/BrainTumorDataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLW284JeJS5B"
      },
      "source": [
        "# **Google Drive connection**\n",
        "\n",
        "Necessary to run the classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y28jDvT0ou8Z",
        "outputId": "6e89a57b-c4e0-4917-db22-3adfc2488f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyGma4rvVH5s"
      },
      "source": [
        "# **Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "thaQRwiA9gLw"
      },
      "outputs": [],
      "source": [
        "# Definisci i percorsi per il set di test, di validazione e di addestramento\n",
        "test_path = \"/content/drive/MyDrive/BrainTumorDataset/Preprocessed/Test\"\n",
        "val_path = \"/content/drive/MyDrive/BrainTumorDataset/Preprocessed/Validation\"\n",
        "train_path = \"/content/drive/MyDrive/BrainTumorDataset/Preprocessed/Train\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1ijH4MAZ1Pk",
        "outputId": "d46dbe37-9423-45df-a099-d9b982b37c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4520 images belonging to 4 classes.\n",
            "Found 652 images belonging to 4 classes.\n",
            "Found 652 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Definisci le dimensioni delle immagini\n",
        "image_size = 250\n",
        "batch_size = 32\n",
        "\n",
        "# Crea un oggetto ImageDataGenerator per il preprocessing delle immagini\n",
        "data_generator = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "# Carica le immagini dal set di addestramento\n",
        "train_generator = data_generator.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(image_size, image_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Carica le immagini dal set di test\n",
        "test_generator = data_generator.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(image_size, image_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Carica le immagini dal set di validazione\n",
        "val_generator = data_generator.flow_from_directory(\n",
        "    val_path,\n",
        "    target_size=(image_size, image_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV_iPmcOQSdN",
        "outputId": "e9fb4f61-dcc7-4a78-da77-f5101358a33c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bilanciamento delle classi nel set di test:\n",
            "Classe glioma_tumor: 28.37423312883436%\n",
            "Classe meningioma_tumor: 28.680981595092025%\n",
            "Classe no_tumor: 15.337423312883436%\n",
            "Classe pituitary_tumor: 27.607361963190186%\n"
          ]
        }
      ],
      "source": [
        "class_indices = test_generator.class_indices\n",
        "num_samples = test_generator.samples\n",
        "\n",
        "class_counts = {class_label: 0 for class_label in class_indices}\n",
        "\n",
        "# Conta i campioni per ogni classe\n",
        "for filename in val_generator.filenames:\n",
        "    class_label = filename.split('/')[0]  # Ottieni l'etichetta della classe dal percorso del file\n",
        "    class_counts[class_label] += 1\n",
        "\n",
        "# Calcola la percentuale di campioni per ogni classe\n",
        "class_balances = {class_label: count / num_samples for class_label, count in class_counts.items()}\n",
        "\n",
        "print(\"Bilanciamento delle classi nel set di test:\")\n",
        "for class_label, balance in class_balances.items():\n",
        "    print(f\"Classe {class_label}: {balance * 100}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPX5AH239cF7"
      },
      "source": [
        "# **STATS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bbT77tXhdvEM"
      },
      "outputs": [],
      "source": [
        "def save_history(name, history):\n",
        "  with open(dataset_path + name, 'wb') as file_pi:\n",
        "    pickle.dump(history.history, file_pi)\n",
        "\n",
        "def load_history(path):\n",
        "  with open(path, \"rb\") as file_pi:\n",
        "    history = pickle.load(file_pi)\n",
        "  return history\n",
        "\n",
        "def train_val_evaluation_plot(history):\n",
        "  acc = history['accuracy']\n",
        "  val_acc = history['val_accuracy']\n",
        "  loss = history['loss']\n",
        "  val_loss = history['val_loss']\n",
        "\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def run_test(model):\n",
        "  m = load_model(dataset_path + model)\n",
        "  # Ottenere le previsioni del modello per i dati di test\n",
        "  y_pred = m.predict(test_generator)\n",
        "  # Convertire le previsioni in classi (etichette) utilizzando la funzione argmax di NumPy\n",
        "  y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "  # Convertire le etichette reali dei dati di test in un array di interi\n",
        "  y_true = test_generator.classes\n",
        "  return y_true, y_pred_classes\n",
        "\n",
        "def stats(model, history):\n",
        "  m = load_model(dataset_path + model)\n",
        "  train_val_evaluation_plot(history)\n",
        "  y_true, y_pred_classes = run_test(model)\n",
        "  print(\"Classification report: \")\n",
        "  print(metrics.classification_report(y_true,y_pred_classes,digits = 4))\n",
        "  metrics.ConfusionMatrixDisplay.from_predictions(y_true, y_pred_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-63kjt0dMZoa"
      },
      "source": [
        "Se il modello riesce ad apprendere correttamente, si dovrebbe osservare un andamento decrescente della loss e un andamento crescente dell'accuracy nel training set. Tuttavia, se la rete ha una capacità eccessiva rispetto al problema da risolvere, può incorrere in overfitting: in tal caso si ha un abbassamento della loss e un aumento dell'accuracy sul training set, ma un peggioramento delle performance sulla validation set (ovvero un aumento della validation loss e un abbassamento della validation accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "7_Zc7qC_90Oe",
        "outputId": "f8c31d6e-b71c-43f2-b461-2d2b134a1a40"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2942a57032c4>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0my_true_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "from itertools import cycle\n",
        "\n",
        "\n",
        "\n",
        "# Calcolare le curve ROC per ogni classe\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "n_classes = test_generator.num_classes\n",
        "\n",
        "y_true_categorical = to_categorical(y_true, num_classes=n_classes)\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_categorical[:, i], y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Calcolare la media delle curve ROC micro e macro\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_categorical.ravel(), y_pred.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# Calcolare il valore medio delle AUC macro\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "mean_tpr /= n_classes\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plot della curva ROC per ogni classe\n",
        "plt.figure()\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='ROC curve (micro-average) (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=2)\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='ROC curve (macro-average) (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=2)\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "             ''.format(i, roc_auc[i]))\n",
        "\n",
        "# Plot della linea diagonale (classificatore casuale)\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "\n",
        "# Impostare limiti e etichette del grafico\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvDy2Ww4AB86"
      },
      "source": [
        "FIN QUI FUNZIONA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3hYTFyL_nri"
      },
      "source": [
        "## TO DO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNIAuVUh4226"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clIhJFr6_08S"
      },
      "source": [
        "## VERSIONE VECCHIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71uM-YC-tjdG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from itertools import cycle\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(4):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green'])\n",
        "for i, color in zip(range(4), colors):\n",
        "    plt.plot(\n",
        "        fpr[i],\n",
        "        tpr[i],\n",
        "        color=color,\n",
        "        lw=2,\n",
        "        label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]),\n",
        "    )\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Some extension of Receiver operating characteristic to multiclass')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7vFD09vnHXJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)#, display_labels=['0', '1'])\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "# classes = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy5Ew-6HL016"
      },
      "source": [
        "confonde meningioma con il glioma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C29MLUU_RCy9"
      },
      "source": [
        "# **CNN from scratch**\n",
        "\n",
        "Se stai aggiungendo 3 layer di convoluzione e Max Pooling alla tua CNN, hai alcune opzioni per le dimensioni delle finestre dei layer di Max Pooling. Puoi lasciare le dimensioni fisse a 3x3 per tutti i layer, oppure puoi renderle incrementali, aumentandole passo dopo passo. Entrambi gli approcci hanno i loro pro e contro.\n",
        "\n",
        "Dimensioni fisse a 3x3 per tutti i layer di Max Pooling: Questo approccio mantiene le dimensioni delle finestre di pooling costanti in tutti i layer. Puoi utilizzare una dimensione di 3x3 per tutti e tre i layer di Max Pooling. Questo offre uniformità nell'estrazione delle caratteristiche e riduce il rischio di perdita di informazioni in modo significativo. Tuttavia, potresti incorrere in una riduzione delle dimensioni spaziali più rapida rispetto a un'alternativa incrementale.\n",
        "\n",
        "Dimensioni incrementali delle finestre di Max Pooling: In alternativa, puoi aumentare le dimensioni delle finestre di pooling man mano che procedi attraverso i layer. Ad esempio, potresti utilizzare una finestra 2x2 per il primo layer di Max Pooling, una finestra 3x3 per il secondo e una finestra 4x4 per il terzo. Questo approccio consente di catturare caratteristiche a diverse scale durante la progressione attraverso la rete. Può essere particolarmente utile se hai immagini di grandi dimensioni o se desideri enfatizzare caratteristiche più ampie a livelli superiori della rete.\n",
        "\n",
        "La scelta tra le due opzioni dipende dal tuo problema specifico, dalle dimensioni delle immagini di input, dalla complessità delle caratteristiche che desideri rilevare e dal trade-off tra dimensione dell'output, complessità computazionale e informazioni mantenute.\n",
        "\n",
        "Tuttavia, ricorda che aumentare le dimensioni delle finestre di pooling può portare a una riduzione più rapida delle dimensioni spaziali dell'output e potrebbe comportare una perdita di dettagli più significativa. Pertanto, è consigliabile eseguire esperimenti e valutare le prestazioni del tuo modello per determinare quale approccio funzioni meglio nel contesto del tuo problema specifico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJalkXRHVjWV"
      },
      "source": [
        "## Exp 1\n",
        "\n",
        "First CNN model with **one convolutional layer** and **one max pooling layer** at the biginning.\n",
        "\n",
        "Than **one flatten layer** and **one dense layer** with 4 units (one for each class) and softmax activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITbZXXbSVafh",
        "outputId": "07d9bc6e-89da-46e9-fdb1-bf5c0d3949fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 250, 250, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 125, 125, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 500000)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 2000004   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,000,900\n",
            "Trainable params: 2,000,900\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(image_size, image_size, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tukT7BgVV3JL"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH6lz1MhV33i",
        "outputId": "9767740d-14bf-4d94-ad6e-d3d104f3de82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "142/142 [==============================] - 1466s 10s/step - loss: 1.5682 - accuracy: 0.5805 - val_loss: 0.7044 - val_accuracy: 0.7531 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "142/142 [==============================] - 257s 2s/step - loss: 0.4011 - accuracy: 0.8626 - val_loss: 0.5778 - val_accuracy: 0.8067 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "142/142 [==============================] - 257s 2s/step - loss: 0.1349 - accuracy: 0.9677 - val_loss: 0.5900 - val_accuracy: 0.8313 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "142/142 [==============================] - 253s 2s/step - loss: 0.0410 - accuracy: 0.9951 - val_loss: 0.6028 - val_accuracy: 0.8298 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "142/142 [==============================] - 253s 2s/step - loss: 0.0155 - accuracy: 0.9993 - val_loss: 0.7274 - val_accuracy: 0.8083 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5)\n",
        "checkpoint = ModelCheckpoint(filepath=dataset_path + '/modello_cnn_exp1.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "callbacks_list = [reduce_lr, checkpoint]\n",
        "history = model.fit(train_generator, epochs=10, validation_data=val_generator, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nSrED0xmk3h3"
      },
      "outputs": [],
      "source": [
        "save_history('/history_cnn_exp1.h5',history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mX4vAcc6dZkb"
      },
      "outputs": [],
      "source": [
        "# load this model and extract statistics\n",
        "stats(\"/modello_cnn_exp1.h5\", load_history(dataset_path + '/history_cnn_exp1.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqmB9r1_b8pH"
      },
      "source": [
        "## Exp 2\n",
        "\n",
        "Second CNN model with **three convolutional layers** and **three max pooling layers** at the biginning.\n",
        "\n",
        "Than **one flatten layer** and **one dense layer** with 256 units and softmax activation function.\n",
        "\n",
        "We add a **dropout layer** with 0.5 probability to avoid overfitting.\n",
        "\n",
        "At the end we add a **dense layer** with 4 units (one for each class) and softmax activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI5PEiR-ce8B"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(image_size, image_size, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((4, 4)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gldp8gGicnoW"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgHpUPewcqva"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5)\n",
        "checkpoint = ModelCheckpoint(filepath=dataset_path + '/modello_cnn_exp2.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "callbacks_list = [reduce_lr, checkpoint]\n",
        "history = model.fit(train_generator, epochs=10, validation_data=val_generator, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akqaO1smlqW7"
      },
      "outputs": [],
      "source": [
        "save_history('/history_cnn_exp2.h5',history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMdhgD9WfLmE"
      },
      "outputs": [],
      "source": [
        "# load this model and extract statistics\n",
        "stats(\"/modello_cnn_exp2.h5\", load_history(dataset_path + '/history_cnn_exp2.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnZjWpwwpyOE"
      },
      "source": [
        "## Exp 3\n",
        "\n",
        "Third CNN model with **four convolutional layers** and **four max pooling layers** at the biginning.\n",
        "\n",
        "Each of the max pooling layer have a **different pool size**.\n",
        "\n",
        "Than **one flatten layer** and **one dense layer** with 256 units and softmax activation function.\n",
        "\n",
        "We add a **dropout layer** with 0.5 probability to avoid overfitting.\n",
        "\n",
        "At the end we add a **dense layer** with 4 units (one for each class) and softmax activation function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FX17rHCp94N"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(image_size, image_size, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((4, 4)))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((5, 5)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CidmxCidqDoC"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52Ds7EasqFZ-"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5)\n",
        "checkpoint = ModelCheckpoint(filepath=dataset_path + '/modello_cnn_exp3.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "callbacks_list = [reduce_lr, checkpoint]\n",
        "history = model.fit(train_generator, epochs=10, validation_data=val_generator, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "982YlD4FqLuW"
      },
      "outputs": [],
      "source": [
        "save_history('/history_cnn_exp3.h5',history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ9vKBf0qONn"
      },
      "outputs": [],
      "source": [
        "# load this model and extract statistics\n",
        "stats(\"/modello_cnn_exp3.h5\", load_history(dataset_path + '/history_cnn_exp3.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imQ45S3--XL-"
      },
      "source": [
        "##Exp 8\n",
        "\n",
        "Eighth CNN model with four convolutional layers and four max pooling layers at the biginning.\n",
        "\n",
        "Now each of the max pooling layer have a different pool size.\n",
        "\n",
        "Than one flatten layer and one dense layer with 512 units and softmax activation function.\n",
        "\n",
        "We add a dropout layer with 0.5 probability to avoid overfitting.\n",
        "\n",
        "At the end we add a dense layer with 4 units (one for each class) and softmax activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWUt-2Ep-jCX"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(image_size, image_size, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((4, 4)))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((5, 5)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWWtoiSt-nFF"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8V9rNSL-oQU"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5)\n",
        "checkpoint = ModelCheckpoint(filepath=dataset_path + '/modello_cnn_exp8.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "callbacks_list = [reduce_lr, checkpoint]\n",
        "history = model.fit(train_generator, epochs=20, validation_data=val_generator, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4Bjqr7p-tAk"
      },
      "outputs": [],
      "source": [
        "save_history('/history_cnn_exp8.h5',history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ELe3uPX-w3N"
      },
      "outputs": [],
      "source": [
        "stats(\"/modello_cnn_exp8.h5\", load_history(dataset_path + '/history_cnn_exp8.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFqP3CBveWEQ"
      },
      "source": [
        "## Exp 9\n",
        "\n",
        "Ninth CNN model with 3 convolutional layers and 3 max pooling layer at the biginning.\n",
        "\n",
        "\n",
        "Now each of the max pooling layer have a different pool size.\n",
        "\n",
        "Than one flatten layer and one dense layer with 512 units and softmax activation function.\n",
        "\n",
        "Than one other dense layer with 256 units and softmax activation function.\n",
        "\n",
        "We add a dropout layer with 0.5 probability to avoid overfitting.\n",
        "\n",
        "At the end we add a dense layer with 4 units (one for each class) and softmax activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knbELMDtefHx"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(image_size, image_size, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((3, 3)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((4, 4)))\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D((5, 5)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlpI7zbKevY2"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQE3YGYqexSY"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5)\n",
        "checkpoint = ModelCheckpoint(filepath=dataset_path + '/modello_cnn_exp9.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "callbacks_list = [reduce_lr, checkpoint]\n",
        "history = model.fit(train_generator, epochs=20, validation_data=val_generator, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rdPxx8r-vrO"
      },
      "outputs": [],
      "source": [
        "save_history('/history_cnn_exp9.h5',history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWmhM6xo-xK4"
      },
      "outputs": [],
      "source": [
        "stats(\"/modello_cnn_exp9.h5\", load_history(dataset_path + '/history_cnn_exp9.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JORPqkMhVW_1"
      },
      "source": [
        "## initial model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBmiGqlc77AK"
      },
      "outputs": [],
      "source": [
        "# Definire un modello sequenziale di Keras\n",
        "model = Sequential()\n",
        "\n",
        "# Aggiungere un layer convoluzionale con 32 filtri, dimensione del kernel di 3x3, funzione di attivazione ReLU e padding \"same\"\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(image_size, image_size, 3)))\n",
        "\n",
        "# Aggiungere un layer di max pooling con dimensione 2x2\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Aggiungere un altro layer convoluzionale con 64 filtri, dimensione del kernel di 3x3, funzione di attivazione ReLU e padding \"same\"\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "# Aggiungere un altro layer di max pooling con dimensione 2x2\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Aggiungere un altro layer convoluzionale con 128 filtri, dimensione del kernel di 3x3, funzione di attivazione ReLU e padding \"same\"\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "# Aggiungere un altro layer di max pooling con dimensione 2x2\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Aggiungere un layer di appiattimento per convertire l'output del layer precedente in un vettore unidimensionale\n",
        "model.add(Flatten())\n",
        "\n",
        "# Aggiungere un layer completamente connesso con 512 unità e funzione di attivazione ReLU\n",
        "model.add(Dense(512, activation='relu'))\n",
        "\n",
        "# Aggiungere un layer di dropout con una percentuale di dropout di 0,5\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Aggiungere un altro layer completamente connesso con 4 unità (una per ogni classe) e funzione di attivazione softmax\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Stampa il riepilogo del modello\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_1FWbFrRri0"
      },
      "outputs": [],
      "source": [
        "# Compilare il modello con la funzione di loss \"categorical_crossentropy\" e l'ottimizzatore \"adam\"\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVcQ0hrEdUgX"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# Definire i callbacks da utilizzare durante l'addestramento del modello\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5)\n",
        "checkpoint = ModelCheckpoint(filepath=dataset_path + '/modello_cnn.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Definire una lista di callbacks\n",
        "callbacks_list = [reduce_lr, checkpoint]\n",
        "\n",
        "# Addestrare il modello usando la lista di callbacks\n",
        "# history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), callbacks=callbacks_list)\n",
        "history = model.fit(train_generator, epochs=10, validation_data=val_generator, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0-lG3ABmxO6"
      },
      "source": [
        "# **Load trained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "YBkBPIZdnB17",
        "outputId": "74d09451-0080-4d84-e63b-c15013c66343"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8edeb17464d8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/modello_cnn.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
          ]
        }
      ],
      "source": [
        "model = load_model(dataset_path + '/modello_cnn.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NDz3ICSorQXo",
        "tLW284JeJS5B",
        "NyGma4rvVH5s",
        "m3hYTFyL_nri",
        "clIhJFr6_08S",
        "XJalkXRHVjWV",
        "XqmB9r1_b8pH",
        "gnZjWpwwpyOE",
        "imQ45S3--XL-",
        "JORPqkMhVW_1",
        "c0-lG3ABmxO6"
      ],
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOP+T5DSzLqomkSlLa+gNAp",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}